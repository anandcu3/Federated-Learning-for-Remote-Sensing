{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run once\n",
    "!conda create -n resnet_fl_2host python=3 anaconda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate conda environment to access pysyft\n",
    "!source /usr/local/anaconda3/bin/activate resnet_fl_2host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary after 2nd run\n",
    "!conda install -c pytorch pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary after 2nd run\n",
    "!pip install syft==0.2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "host1 = sy.VirtualWorker(hook, id=\"host1\")\n",
    "host2 = sy.VirtualWorker(hook, id=\"host2\")\n",
    "data_dir = \"UCMerced_LandUse/Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n",
      "{'num_workers': 1, 'pin_memory': True}\n"
     ]
    }
   ],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 8\n",
    "        self.test_batch_size = 8\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = True\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(device)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "def load_split_train_test(datadir, valid_size = .2):\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "   \n",
    "    loaded_dataset = datasets.ImageFolder(data_dir, transform=train_transforms)\n",
    "    train_set, val_set = torch.utils.data.random_split(loaded_dataset, [1680, 420])\n",
    "    num_train = len(loaded_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    federated_train_loader = sy.FederatedDataLoader(\n",
    "        loaded_dataset.federate((host1, host2)),\n",
    "        batch_size=args.batch_size, \n",
    "        **kwargs\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        loaded_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        #shuffle=True, \n",
    "        **kwargs\n",
    "    )\n",
    "    federated_test_loader = torch.utils.data.DataLoader(\n",
    "        loaded_dataset,\n",
    "        batch_size=args.test_batch_size,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return {'train': federated_train_loader, 'val': federated_test_loader}, {'train': len(train_idx), 'val':len(test_idx)}, loaded_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The following options are not supported: num_workers: 1, pin_memory: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fa4cd4d8070> {'train': 1680, 'val': 420} ['agricultural', 'airplane', 'baseballdiamond', 'beach', 'buildings', 'chaparral', 'denseresidential', 'forest', 'freeway', 'golfcourse', 'harbor', 'intersection', 'mediumresidential', 'mobilehomepark', 'overpass', 'parkinglot', 'river', 'runway', 'sparseresidential', 'storagetanks', 'tenniscourt']\n"
     ]
    }
   ],
   "source": [
    "dataloaders, dataset_sizes, class_names = load_split_train_test(data_dir, .2)\n",
    "print(dataloaders['val'], dataset_sizes, class_names)\n",
    "#class_names = dataloaders['val'].dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef test(args, model, device, test_loader):\\n    model.eval()\\n    test_loss = 0\\n    correct = 0\\n    with torch.no_grad():\\n        for data, target in test_loader:\\n            data, target = data.to(device), target.to(device)\\n            output = model(data)\\n            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\\n            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \\n            correct += pred.eq(target.view_as(pred)).sum().item()\\n\\n    test_loss /= len(test_loader.dataset)\\n\\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\\n        test_loss, correct, len(test_loader.dataset),\\n        100. * correct / len(test_loader.dataset)))\\n    \""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))\n",
    "'''\n",
    "'''\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, num_epochs=5, batch_size=4):\n",
    "    %matplotlib inline\n",
    "    import pylab as pl\n",
    "    from IPython import display\n",
    "    def live_plot(data):\n",
    "        pl.plot(data)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(pl.gcf())\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    loss_values, accuracy_values, loss_values_val, accuracy_values_val = [], [], [], []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss_train, running_loss_val = 0, 0\n",
    "        running_corrects_train, running_corrects_val = 0, 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i, (inputs, labels) in enumerate(tqdm(dataloaders['train'])):\n",
    "            # NEW) send model to correct worker\n",
    "            if i>=1680:\n",
    "                break\n",
    "            model.send(inputs.location)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # backward + optimize only if in training phase\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            # get model (with gradients)\n",
    "            model.get()\n",
    "            # statistics\n",
    "            running_loss_train += loss.get().item() * batch_size\n",
    "            running_corrects_train += torch.sum(preds == labels.data).get()\n",
    "            #print(preds.get(),labels.get())\n",
    "            #running_loss_train += loss.item() * batch_size\n",
    "            #running_corrects_train += torch.sum(preds == labels.data)\n",
    "        #scheduler.step()\n",
    "        \n",
    "        epoch_loss_train = running_loss_train / (dataset_sizes['train']+dataset_sizes['val'])\n",
    "        epoch_acc_train = running_corrects_train.double() / (dataset_sizes['train']+dataset_sizes['val'])\n",
    "        loss_values.append(epoch_loss_train)\n",
    "        accuracy_values.append(epoch_acc_train.item())\n",
    "        live_plot(loss_values)\n",
    "        print(loss_values,)\n",
    "        print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch_loss_train, epoch_acc_train))\n",
    "\n",
    "        model.eval()   # Set model to evaluate mode\n",
    "        for i, (inputs, labels) in enumerate(tqdm(dataloaders['train'])):\n",
    "            if i<=1680:\n",
    "                continue\n",
    "            model.send(inputs.location)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss_val += loss.get() * batch_size\n",
    "            running_corrects_val += torch.sum(preds == labels.data).get()\n",
    "            #running_loss_val += loss * batch_size\n",
    "            #running_corrects_val += torch.sum(preds == labels.data)\n",
    "            model.get()\n",
    "        epoch_loss_val = running_loss_val / dataset_sizes['val']\n",
    "        epoch_acc_val = running_corrects_val / dataset_sizes['val']\n",
    "        loss_values_val.append(epoch_loss_val)\n",
    "        accuracy_values_val.append(epoch_acc_val)\n",
    "        print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(epoch_loss_val, epoch_acc_val))\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_acc_train > best_acc:\n",
    "            best_acc = epoch_acc_train\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    #print(loss_values, accuracy_values, loss_values_val, accuracy_values_val)\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, {\"tloss\":loss_values, \"valloss\":loss_values_val, \"tacc\":accuracy_values,\"valacc\":accuracy_values_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        num_ftrs = resnet.fc.in_features\n",
    "        resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(num_ftrs, n_classes)\n",
    "        )\n",
    "        self.base_model = resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        from collections import OrderedDict\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(5, 5))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(5, 5))\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(5, 5))\n",
    "        #self.conv4 = nn.Conv2d(64, 128, kernel_size=(5, 5))\n",
    "        self.linear1 = nn.Linear(64 * 24 * 24, 120)\n",
    "        self.linear2 = nn.Linear(120, 84)\n",
    "        self.linear3 = nn.Linear(84, n_classes)                                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, 1, 28, 28): Input images.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, 10): Outputs of the network.\n",
    "        \"\"\"\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), kernel_size=2, stride=2)\n",
    "        #x = F.max_pool2d(F.relu(self.conv4(x)), kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 64 * 24 * 24)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "model = CNN(len(class_names))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, stats_dict = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, num_epochs=25, batch_size=args.batch_size)\n",
    "torch.save(model.state_dict(), \"resnet_fl2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (linear1): Linear(in_features=36864, out_features=120, bias=True)\n",
      "  (linear2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (linear3): Linear(in_features=84, out_features=21, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
