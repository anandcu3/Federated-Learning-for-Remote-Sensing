{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run once\n",
    "!conda create -n resnet_fl_2host python=3 anaconda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate conda environment to access pysyft\n",
    "!source /usr/local/anaconda3/bin/activate resnet_fl_2host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# not necessary after 2nd run\n",
    "!conda install -c pytorch pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary after 2nd run\n",
    "!pip install syft==0.2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "#torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "host1 = sy.VirtualWorker(hook, id=\"host1\")\n",
    "host2 = sy.VirtualWorker(hook, id=\"host2\")\n",
    "data_dir = \"UCMerced_LandUse/Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 4\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = True\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(device)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "def load_split_train_test(datadir, valid_size = .2):\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    loaded_dataset = datasets.ImageFolder(data_dir, transform=train_transforms)\n",
    "    \n",
    "    num_train = len(loaded_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    federated_train_loader = sy.FederatedDataLoader(\n",
    "        loaded_dataset.federate((host1, host2)),\n",
    "        batch_size=args.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        #shuffle=True, \n",
    "        **kwargs\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        loaded_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        #shuffle=True, \n",
    "        **kwargs\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        loaded_dataset,\n",
    "        batch_size=args.test_batch_size,\n",
    "        sampler=test_sampler,       \n",
    "        #shuffle=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return {'train': federated_train_loader, 'val': test_loader}, {'train': len(train_idx), 'val':len(test_idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The following options are not supported: sampler: <torch.utils.data.sampler.SubsetRandomSampler object at 0x7f18855269a0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agricultural', 'airplane', 'baseballdiamond', 'beach', 'buildings', 'chaparral', 'denseresidential', 'forest', 'freeway', 'golfcourse', 'harbor', 'intersection', 'mediumresidential', 'mobilehomepark', 'overpass', 'parkinglot', 'river', 'runway', 'sparseresidential', 'storagetanks', 'tenniscourt'] {'train': 1680, 'val': 420}\n"
     ]
    }
   ],
   "source": [
    "dataloaders, dataset_sizes = load_split_train_test(data_dir, .2)\n",
    "print(dataloaders['val'].dataset.classes, dataset_sizes)\n",
    "class_names = dataloaders['val'].dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))\n",
    "'''\n",
    "'''\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, num_epochs=5, batch_size=4):\n",
    "    %matplotlib inline\n",
    "    import pylab as pl\n",
    "    from IPython import display\n",
    "    \n",
    "    def live_plot(data):\n",
    "        pl.plot(data)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(pl.gcf())\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    loss_values = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss_train, running_loss_val = 0, 0\n",
    "        running_corrects_train, running_corrects_val = 0, 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in tqdm(dataloaders['train']):\n",
    "            # NEW) send model to correct worker\n",
    "            model.send(inputs.location)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # backward + optimize only if in training phase\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            # get model (with gradients)\n",
    "            model.get()\n",
    "            # statistics\n",
    "            running_loss_train += loss.get().item() * batch_size\n",
    "            running_corrects_train += torch.sum(preds == labels.data).get()\n",
    "            #running_loss_train += loss.item() * batch_size\n",
    "            #running_corrects_train += torch.sum(preds == labels.data)\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss_train = running_loss_train / dataset_sizes['train']\n",
    "        epoch_acc_train = running_corrects_train.double() / dataset_sizes['train']\n",
    "        loss_values.append(epoch_loss_train)\n",
    "        live_plot(loss_values)\n",
    "        print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch_loss_train, epoch_acc_train))\n",
    "\n",
    "        model.eval()   # Set model to evaluate mode\n",
    "        for inputs, labels in tqdm(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss_val += loss.item() * batch_size\n",
    "            running_corrects_val += torch.sum(preds == labels.data)\n",
    "        epoch_loss_val = running_loss_val / dataset_sizes['val']\n",
    "        epoch_acc_val = running_corrects_val.double() / dataset_sizes['val']\n",
    "        print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(epoch_loss_val, epoch_acc_val))\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_acc_val > best_acc:\n",
    "            best_acc = epoch_acc_val\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        num_ftrs = resnet.fc.in_features\n",
    "        resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(num_ftrs, n_classes)\n",
    "        )\n",
    "        self.base_model = resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        from collections import OrderedDict\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(5, 5))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(5, 5))\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(5, 5))\n",
    "        self.linear1 = nn.Linear(64 * 24 * 24, 120)\n",
    "        self.linear2 = nn.Linear(120, 84)\n",
    "        self.linear3 = nn.Linear(84, n_classes)                                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, 1, 28, 28): Input images.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, 10): Outputs of the network.\n",
    "        \"\"\"\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 64 * 24 * 24)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "model = CNN(len(class_names))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2832b97586d34e87b07713bd8b756352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8186cde5644036afb2047c697b859f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=525.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, num_epochs=20, batch_size=args.batch_size)\n",
    "torch.save(model.state_dict(), \"resnet_fl2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2112 (0%)]\tLoss: -0.493185\n",
      "Train Epoch: 1 [1920/2112 (91%)]\tLoss: -0.518956\n",
      "\n",
      "Test set: Average loss: -0.5051, Accuracy: 120/2100 (6%)\n",
      "\n",
      "Train Epoch: 2 [0/2112 (0%)]\tLoss: -0.488512\n",
      "Train Epoch: 2 [1920/2112 (91%)]\tLoss: -0.496494\n",
      "\n",
      "Test set: Average loss: -0.5063, Accuracy: 101/2100 (5%)\n",
      "\n",
      "Train Epoch: 3 [0/2112 (0%)]\tLoss: -0.514812\n",
      "Train Epoch: 3 [1920/2112 (91%)]\tLoss: -0.513043\n",
      "\n",
      "Test set: Average loss: -0.5055, Accuracy: 97/2100 (5%)\n",
      "\n",
      "Train Epoch: 4 [0/2112 (0%)]\tLoss: -0.508560\n",
      "Train Epoch: 4 [1920/2112 (91%)]\tLoss: -0.487546\n",
      "\n",
      "Test set: Average loss: -0.5041, Accuracy: 98/2100 (5%)\n",
      "\n",
      "Train Epoch: 5 [0/2112 (0%)]\tLoss: -0.516424\n",
      "Train Epoch: 5 [1920/2112 (91%)]\tLoss: -0.531590\n",
      "\n",
      "Test set: Average loss: -0.5080, Accuracy: 101/2100 (5%)\n",
      "\n",
      "Train Epoch: 6 [0/2112 (0%)]\tLoss: -0.474073\n",
      "Train Epoch: 6 [1920/2112 (91%)]\tLoss: -0.528637\n",
      "\n",
      "Test set: Average loss: -0.5071, Accuracy: 101/2100 (5%)\n",
      "\n",
      "Train Epoch: 7 [0/2112 (0%)]\tLoss: -0.516183\n",
      "Train Epoch: 7 [1920/2112 (91%)]\tLoss: -0.534867\n",
      "\n",
      "Test set: Average loss: -0.5065, Accuracy: 102/2100 (5%)\n",
      "\n",
      "Train Epoch: 8 [0/2112 (0%)]\tLoss: -0.524293\n",
      "Train Epoch: 8 [1920/2112 (91%)]\tLoss: -0.474074\n",
      "\n",
      "Test set: Average loss: -0.5060, Accuracy: 100/2100 (5%)\n",
      "\n",
      "Train Epoch: 9 [0/2112 (0%)]\tLoss: -0.513911\n",
      "Train Epoch: 9 [1920/2112 (91%)]\tLoss: -0.519910\n",
      "\n",
      "Test set: Average loss: -0.5067, Accuracy: 106/2100 (5%)\n",
      "\n",
      "Train Epoch: 10 [0/2112 (0%)]\tLoss: -0.493948\n",
      "Train Epoch: 10 [1920/2112 (91%)]\tLoss: -0.498887\n",
      "\n",
      "Test set: Average loss: -0.5081, Accuracy: 112/2100 (5%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, federated_train_loader, optimizer_ft, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"resnet_fl.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
