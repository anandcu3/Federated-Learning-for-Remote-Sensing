{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run once\n",
    "#!conda create -n resnet_fl_2host python=3 anaconda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate conda environment to access pysyft\n",
    "!source /usr/local/anaconda3/bin/activate resnet_fl_2host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natsort\n",
      "  Downloading natsort-7.1.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: natsort\n",
      "Successfully installed natsort-7.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import syft as sy\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 4\n",
    "        self.test_batch_size = 100\n",
    "        self.epochs = 5\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = True\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel ('../multilabels/LandUse_Multilabeled.xlsx')\n",
    "df_label = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names =  np.array([\"airplane\",\"bare-soil\",\"buildings\",\"cars\",\"chaparral\",\"court\",\"dock\",\"field\",\"grass\",\"mobile-home\",\"pavement\",\"sand\",\"sea\",\"ship\",\"tanks\",\"trees\",\"water\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674 bare-soil cars (2, 4)\n"
     ]
    }
   ],
   "source": [
    "largestxor = 0 \n",
    "largestij = (0,0)\n",
    "\n",
    "for i in range(1,17):\n",
    "    for j in range(i+1,18):\n",
    "        #colnand = np.sum(np.logical_not(np.logical_and(df_label[:,i], df_label[:,j])))\n",
    "        colxor = np.sum(np.logical_xor(df_label[:,i].astype(bool) , df_label[:,j].astype(bool) )) -  np.sum(np.logical_and(df_label[:,i], df_label[:,j]))\n",
    "        #print(i,j, colxor, colnand)\n",
    "        if colxor >= largestxor and np.sum(df_label[:,i]) >=700 and np.sum(df_label[:,j])>= 700 :\n",
    "            largestxor = colxor\n",
    "            largestij = (i,j)\n",
    "print(largestxor,class_names[largestij[0]-1], class_names[largestij[1]-1], largestij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncor_selecter(nr_label = 4,min_img = 300):\n",
    "    \"\"\"retrun a list with the least correlated labels \"\"\"\n",
    "    image_perlabel = np.sum(df_label[:,1:],axis= 0)\n",
    "    biggest_label =np.where(np.any([image_perlabel > min_img],axis=0))[0]\n",
    "    #print(biggest_label, image_perlabel[biggest_label])\n",
    "\n",
    "    selected_list = [] \n",
    "    allcor_lost = np.array([0,0,0])\n",
    "    for i in range(0,len(biggest_label)-1):\n",
    "        it = biggest_label[i]\n",
    "        for j in range(i+1,len(biggest_label)):\n",
    "            jt = biggest_label[j]\n",
    "\n",
    "            colxor = np.sum(np.logical_xor(df_label[:,it].astype(bool) , df_label[:,jt].astype(bool) )) -  np.sum(np.logical_and(df_label[:,it], df_label[:,jt]))\n",
    "            allcor_lost = np.vstack((allcor_lost, np.array([colxor,it,jt]))) \n",
    "    sorted_list = allcor_lost[allcor_lost[:,0].argsort()]\n",
    "    selected_list.append(sorted_list[-1,1])\n",
    "    selected_list.append(sorted_list[-1,2])\n",
    "    #print(sorted_list, selected_list)        \n",
    "\n",
    "    while len(selected_list)<nr_label:\n",
    "        biggest_label = np.setdiff1d(biggest_label,np.array(selected_list))\n",
    "        largestxor = 0 \n",
    "        largestind = 0\n",
    "        for i in biggest_label:\n",
    "            overall_xor = 0 \n",
    "            for j in (selected_list):\n",
    "                overall_xor += np.sum(np.logical_xor(df_label[:,i].astype(bool) , df_label[:,j].astype(bool) )) -  np.sum(np.logical_and(df_label[:,i], df_label[:,j]))\n",
    "\n",
    "            if overall_xor >= largestxor:\n",
    "                largestxor = overall_xor\n",
    "                largestind = i\n",
    "\n",
    "        selected_list.append(largestind)\n",
    "    \n",
    "    return selected_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sampler_split_for_client(cdata, idxs, nr_client=4, minimum_skew_percentage = .4):\n",
    "    selected_labels = uncor_selecter(nr_client,300)\n",
    "    \n",
    "    splitlists = []\n",
    "    for sb in selected_labels:\n",
    "        splitlists.append([])\n",
    "        \n",
    "    \n",
    "    for i in idxs:\n",
    "        nplabel = cdata.__getlabel__(i)\n",
    "        #nplabel = label.numpy()\n",
    "        \n",
    "        if np.any(nplabel[selected_labels] == 1):\n",
    "            if random.random() < minimum_skew_percentage:\n",
    "                \n",
    "                flip = np.random.randint(np.sum(nplabel[selected_labels] == 1)) \n",
    "                mask = np.where(nplabel[selected_labels] == 1)[0][flip]\n",
    "                splitlists[mask].append(i)\n",
    "            \n",
    "            else:\n",
    "                flip = np.random.randint(nr_client) \n",
    "                splitlists[flip].append(i)\n",
    "                    \n",
    "        else:\n",
    "            flip = np.random.randint(nr_client) \n",
    "            splitlists[flip].append(i)\n",
    "\n",
    "    \n",
    "    for alist in splitlists:\n",
    "        print(len(alist))\n",
    "    return splitlists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from natsort import natsorted\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, main_dir, transform, labelmat):\n",
    "        self.main_dir = main_dir\n",
    "        self.transforms = transform\n",
    "        self.all_imgs = glob.glob(os.path.join(main_dir, '**/*.tif'), recursive=True)\n",
    "        self.total_imgs = natsorted(self.all_imgs)\n",
    "        self.xlabels = labelmat\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(idx,len(self.total_imgs))\n",
    "        img_loc = self.total_imgs[idx]\n",
    "        #print(img_loc)\n",
    "        imagebaselabel = os.path.splitext(os.path.basename(img_loc))[0]\n",
    "        label = self.xlabels[np.where(self.xlabels[:,0] == imagebaselabel),1:].reshape(17).astype(np.int64)\n",
    "        #print(label,label.shape)\n",
    "        tensor_label =  torch.from_numpy(label)\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transforms(image)\n",
    "        return tensor_image, tensor_label\n",
    "    \n",
    "    def __getlabel__(self, idx):\n",
    "        \n",
    "        img_loc = self.total_imgs[idx]\n",
    "        #print(img_loc)\n",
    "        imagebaselabel = os.path.splitext(os.path.basename(img_loc))[0]\n",
    "        label = self.xlabels[np.where(self.xlabels[:,0] == imagebaselabel),1:].reshape(17).astype(np.int64)\n",
    "        \n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../datasets/UCMerced_LandUse/Images\"\n",
    "\n",
    "def load_split_train_test(datadir, labelmat, valid_size=.2, num_clients=4):\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_data = CustomDataSet(datadir, transform=train_transforms, labelmat=labelmat)\n",
    "    test_data = CustomDataSet(datadir, transform=train_transforms, labelmat=labelmat)\n",
    "\n",
    "    indices = list(range(2100))\n",
    "    split = int(np.floor(valid_size * 2100))\n",
    "    np.random.shuffle(indices)\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    lists = sampler_split_for_client(train_data, train_idx, num_clients, .4)\n",
    "    \n",
    "    dataloaders = []\n",
    "    for client_sampler in lists:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=args.batch_size,\n",
    "            sampler=torch.utils.data.Sampler(client_sampler),\n",
    "            **kwargs\n",
    "        )\n",
    "        dataloaders.append( {'data': train_loader, 'size': len(client_sampler)} )\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n",
      "303\n",
      "332\n",
      "355\n",
      "375\n",
      "[{'data': <torch.utils.data.dataloader.DataLoader object at 0x7f96cb1e6ed0>, 'size': 315}, {'data': <torch.utils.data.dataloader.DataLoader object at 0x7f96cb1e6ad0>, 'size': 303}, {'data': <torch.utils.data.dataloader.DataLoader object at 0x7f96cb1e62d0>, 'size': 332}, {'data': <torch.utils.data.dataloader.DataLoader object at 0x7f96cb043250>, 'size': 355}, {'data': <torch.utils.data.dataloader.DataLoader object at 0x7f96cb043210>, 'size': 375}]\n"
     ]
    }
   ],
   "source": [
    "clients_listo = load_split_train_test(data_dir, df_label, .2, 5)\n",
    "print(clients_listo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_client_model(args, model, device, client_dataloader, optimizer, criterion, scheduler, local_epochs):\n",
    "    \n",
    "    # train\n",
    "    for epoch in range(local_epochs):\n",
    "        \n",
    "        running_loss_train, running_loss_val = 0, 0\n",
    "        running_corrects_train, running_corrects_val = 0, 0\n",
    "        \n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        for data, target in client_dataloader['data']:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss_train += loss.item() * args.batch_size\n",
    "            running_corrects_train += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss_train = running_loss_train / client_dataloader['size']\n",
    "        epoch_acc_train = running_corrects_train.double() /  client_dataloader['size']\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_EPOCHS = 5\n",
    "C_FRACTION = 0.7\n",
    "\n",
    "#clients is array of dataloaders\n",
    "def train_fedavg_model(args, model, device, clients, optimizer, criterion, scheduler, c_fraction, epochs=10):\n",
    "    # iterate through epochs\n",
    "    for i in range(epochs):\n",
    "        # get random subset of clients\n",
    "        fraction = int( c_fraction * float(len(clients_t)) )\n",
    "        client_subset = random.sample(clients, fraction)\n",
    "        \n",
    "        # train each of the clients\n",
    "        models = []\n",
    "        print(\"Running epoch numero \" + str(i))\n",
    "        for client in client_subset:\n",
    "            client_model = train_client_model(args, model, device, client, optimizer, criterion, scheduler, LOCAL_EPOCHS)\n",
    "            models.append(client_model)\n",
    "            print(\"Done with clientelo numero whateva\")\n",
    "            \n",
    "        # average clients params\n",
    "        # model = sum(k for 1 - num_clients): ( data_client / total_num_data ) * model_client_k\n",
    "    \n",
    "    return models[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LENET(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(LENET, self).__init__()\n",
    "        from collections import OrderedDict\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(5, 5))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(5, 5))\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(5, 5))\n",
    "        #self.conv4 = nn.Conv2d(64, 128, kernel_size=(5, 5))\n",
    "        self.linear1 = nn.Linear(64 * 24 * 24, 120)\n",
    "        self.linear2 = nn.Linear(120, 84)\n",
    "        self.linear3 = nn.Linear(84, n_classes)                                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, 1, 28, 28): Input images.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, 10): Outputs of the network.\n",
    "        \"\"\"\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), kernel_size=2, stride=2)\n",
    "        #x = F.max_pool2d(F.relu(self.conv4(x)), kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 64 * 24 * 24)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LENET(len(class_names))\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch numero 0\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-48b15b3dceb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fedavg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients_listo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_FRACTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-52878028c52a>\u001b[0m in \u001b[0;36mtrain_fedavg_model\u001b[0;34m(args, model, device, clients, optimizer, criterion, scheduler, c_fraction, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running epoch numero \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient_subset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mclient_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_client_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOCAL_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done with clientelo numero whateva\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-1dda40e12d5a>\u001b[0m in \u001b[0;36mtrain_client_model\u001b[0;34m(args, model, device, client_dataloader, optimizer, criterion, scheduler, local_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient_dataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_fedavg_model(args, model, device, clients_listo, optimizer_ft, criterion, exp_lr_scheduler, C_FRACTION, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
