{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run once\n",
    "#!conda create -n resnet_fl_2host python=3 anaconda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate conda environment to access pysyft\n",
    "!source /usr/local/anaconda3/bin/activate resnet_fl_2host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natsort\n",
      "  Downloading natsort-7.1.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: natsort\n",
      "Successfully installed natsort-7.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/anaconda3/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.4.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import syft as sy\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 4\n",
    "        self.test_batch_size = 100\n",
    "        self.epochs = 5\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = True\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel ('../multilabels/LandUse_Multilabeled.xlsx')\n",
    "df_label = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names =  np.array([\"airplane\",\"bare-soil\",\"buildings\",\"cars\",\"chaparral\",\"court\",\"dock\",\"field\",\"grass\",\"mobile-home\",\"pavement\",\"sand\",\"sea\",\"ship\",\"tanks\",\"trees\",\"water\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674 bare-soil cars (2, 4)\n"
     ]
    }
   ],
   "source": [
    "largestxor = 0 \n",
    "largestij = (0,0)\n",
    "\n",
    "for i in range(1,17):\n",
    "    for j in range(i+1,18):\n",
    "        #colnand = np.sum(np.logical_not(np.logical_and(df_label[:,i], df_label[:,j])))\n",
    "        colxor = np.sum(np.logical_xor(df_label[:,i].astype(bool) , df_label[:,j].astype(bool) )) -  np.sum(np.logical_and(df_label[:,i], df_label[:,j]))\n",
    "        #print(i,j, colxor, colnand)\n",
    "        if colxor >= largestxor and np.sum(df_label[:,i]) >=700 and np.sum(df_label[:,j])>= 700 :\n",
    "            largestxor = colxor\n",
    "            largestij = (i,j)\n",
    "print(largestxor,class_names[largestij[0]-1], class_names[largestij[1]-1], largestij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncor_selecter(nr_label = 4,min_img = 300):\n",
    "    \"\"\"retrun a list with the least correlated labels \"\"\"\n",
    "    image_perlabel = np.sum(df_label[:,1:],axis= 0)\n",
    "    biggest_label =np.where(np.any([image_perlabel > min_img],axis=0))[0]\n",
    "    #print(biggest_label, image_perlabel[biggest_label])\n",
    "\n",
    "    selected_list = [] \n",
    "    allcor_lost = np.array([0,0,0])\n",
    "    for i in range(0,len(biggest_label)-1):\n",
    "        it = biggest_label[i]\n",
    "        for j in range(i+1,len(biggest_label)):\n",
    "            jt = biggest_label[j]\n",
    "\n",
    "            colxor = np.sum(np.logical_xor(df_label[:,it].astype(bool) , df_label[:,jt].astype(bool) )) -  np.sum(np.logical_and(df_label[:,it], df_label[:,jt]))\n",
    "            allcor_lost = np.vstack((allcor_lost, np.array([colxor,it,jt]))) \n",
    "    sorted_list = allcor_lost[allcor_lost[:,0].argsort()]\n",
    "    selected_list.append(sorted_list[-1,1])\n",
    "    selected_list.append(sorted_list[-1,2])\n",
    "    #print(sorted_list, selected_list)        \n",
    "\n",
    "    while len(selected_list)<nr_label:\n",
    "        biggest_label = np.setdiff1d(biggest_label,np.array(selected_list))\n",
    "        largestxor = 0 \n",
    "        largestind = 0\n",
    "        for i in biggest_label:\n",
    "            overall_xor = 0 \n",
    "            for j in (selected_list):\n",
    "                overall_xor += np.sum(np.logical_xor(df_label[:,i].astype(bool) , df_label[:,j].astype(bool) )) -  np.sum(np.logical_and(df_label[:,i], df_label[:,j]))\n",
    "\n",
    "            if overall_xor >= largestxor:\n",
    "                largestxor = overall_xor\n",
    "                largestind = i\n",
    "\n",
    "        selected_list.append(largestind)\n",
    "    \n",
    "    return selected_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sampler_split_for_client(cdata, idxs, nr_client=4, minimum_skew_percentage = .2):\n",
    "    selected_labels = uncor_selecter(nr_client,300)\n",
    "    \n",
    "    splitlists = []\n",
    "    for sb in selected_labels:\n",
    "        splitlists.append([])\n",
    "        \n",
    "    \n",
    "    for i in idxs:\n",
    "        nplabel = cdata.__getlabel__(i)\n",
    "        #nplabel = label.numpy()\n",
    "        \n",
    "        if np.any(nplabel[selected_labels] == 1):\n",
    "            if random.random() < minimum_skew_percentage:\n",
    "                \n",
    "                flip = np.random.randint(np.sum(nplabel[selected_labels] == 1)) \n",
    "                mask = np.where(nplabel[selected_labels] == 1)[0][flip]\n",
    "                splitlists[mask].append(i)\n",
    "            \n",
    "            else:\n",
    "                flip = np.random.randint(nr_client) \n",
    "                splitlists[flip].append(i)\n",
    "                    \n",
    "        else:\n",
    "            flip = np.random.randint(nr_client) \n",
    "            splitlists[flip].append(i)\n",
    "\n",
    "    \n",
    "    for alist in splitlists:\n",
    "        print(len(alist))\n",
    "    return splitlists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from natsort import natsorted\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, main_dir, transform, labelmat):\n",
    "        self.main_dir = main_dir\n",
    "        self.transforms = transform\n",
    "        self.all_imgs = glob.glob(os.path.join(main_dir, '**/*.tif'), recursive=True)\n",
    "        self.total_imgs = natsorted(self.all_imgs)\n",
    "        self.xlabels = labelmat\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(idx,len(self.total_imgs))\n",
    "        img_loc = self.total_imgs[idx]\n",
    "        #print(img_loc)\n",
    "        imagebaselabel = os.path.splitext(os.path.basename(img_loc))[0]\n",
    "        label = self.xlabels[np.where(self.xlabels[:,0] == imagebaselabel),1:].reshape(17).astype(np.int64)\n",
    "        #print(label,label.shape)\n",
    "        tensor_label =  torch.from_numpy(label)\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transforms(image)\n",
    "        return tensor_image, tensor_label\n",
    "    \n",
    "    def __getlabel__(self, idx):\n",
    "        \n",
    "        img_loc = self.total_imgs[idx]\n",
    "        #print(img_loc)\n",
    "        imagebaselabel = os.path.splitext(os.path.basename(img_loc))[0]\n",
    "        label = self.xlabels[np.where(self.xlabels[:,0] == imagebaselabel),1:].reshape(17).astype(np.int64)\n",
    "        \n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../datasets/UCMerced_LandUse/Images\"\n",
    "\n",
    "def load_split_train_test(datadir, labelmat, valid_size=.2, num_clients=4):\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_data = CustomDataSet(datadir, transform=train_transforms, labelmat=labelmat)\n",
    "    test_data = CustomDataSet(datadir, transform=train_transforms, labelmat=labelmat)\n",
    "\n",
    "    indices = list(range(2100))\n",
    "    split = int(np.floor(valid_size * 2100))\n",
    "    np.random.shuffle(indices)\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    lists = sampler_split_for_client(train_data, train_idx, num_clients, .4)\n",
    "    \n",
    "    # test dataset\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=4)\n",
    "    test_loader_dict = { 'data': test_loader, 'size': len(test_sampler) }\n",
    "    \n",
    "    dataloaders = []\n",
    "    for client_sampler in lists:\n",
    "        train_sampler = SubsetRandomSampler(client_sampler)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=args.batch_size\n",
    "        )\n",
    "        dataloaders.append( {'data': train_loader, 'size': len(client_sampler)} )\n",
    "    \n",
    "    return dataloaders, test_loader_dict, len(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n",
      "326\n",
      "301\n",
      "339\n",
      "408\n"
     ]
    }
   ],
   "source": [
    "clients_listo, valloader, train_len = load_split_train_test(data_dir, df_label, .2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_client_model(args, model, device, client_dataloader, optimizer, criterion, scheduler, local_epochs):\n",
    "    \n",
    "    # train\n",
    "    for epoch in range(local_epochs):\n",
    "        \n",
    "        running_loss_train, running_loss_val = 0, 0\n",
    "        running_corrects_train, running_corrects_val = 0, 0\n",
    "        \n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        for data, target in client_dataloader['data']:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss_train += loss.item() * args.batch_size\n",
    "            running_corrects_train += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss_train = running_loss_train / client_dataloader['size']\n",
    "        epoch_acc_train = running_corrects_train.double() /  client_dataloader['size']\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=5, phase = 'train'):\n",
    "    tloss, tacc= [] , []\n",
    "    vloss, vacc= [] , []\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    #best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        if True:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders['data']:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                    outputcpu = outputs.cpu()\n",
    "                    preds = np.heaviside(outputcpu.detach().numpy(),0)\n",
    "                    #print(outputs, preds)\n",
    "                    loss = criterion(outputs, labels.type(torch.float))\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                    #outputsnp = outputs.cpu().numpy()\n",
    "                    #preds = np.array(outputsnp > 0.5, dtype=float)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += ((torch.sum(torch.from_numpy(preds).to(device) == labels.data)).item() / len(class_names))\n",
    "                #print(\"running_corrects\",running_corrects)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataloaders['size']\n",
    "            epoch_acc = (running_corrects) / dataloaders['size']\n",
    "            \n",
    "            if phase == 'train':\n",
    "                tloss.append(epoch_loss)\n",
    "                tacc.append(epoch_acc)\n",
    "            \n",
    "            if phase == 'val':\n",
    "                vloss.append(epoch_loss)\n",
    "                vacc.append(epoch_acc)\n",
    "            \n",
    "            #print(dataset_sizes[phase],epoch_acc)\n",
    "            #print(type(epoch_loss),type(epoch_acc))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return copy.deepcopy(model),[tloss,tacc,vloss,vacc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_EPOCHS = 5\n",
    "C_FRACTION = 0.7\n",
    "\n",
    "#clients is array of dataloaders\n",
    "def train_fedavg_model(model, device, clients, optimizer, criterion, scheduler, c_fraction, epochs=10):\n",
    "    \n",
    "    # initial model\n",
    "    init_model = copy.deepcopy(model)\n",
    "    \n",
    "    # iterate through epochs\n",
    "    for i in range(epochs):\n",
    "        # get random subset of clients\n",
    "        fraction = int( c_fraction * float(len(clients)) )\n",
    "        client_subset = random.sample(clients, fraction)\n",
    "        \n",
    "        # train each of the clients\n",
    "        model_client_list = []\n",
    "        print(\"Running epoch numero \" + str(i))\n",
    "        for client in client_subset:\n",
    "            model_for_client = copy.deepcopy(model)\n",
    "            client_model, statistics = train_model(model_for_client, client, criterion, optimizer, scheduler, num_epochs=1, phase = 'train')\n",
    "            model_client_list.append(client_model)\n",
    "            print(\"Done with clientelo numero x with stats: \", statistics)\n",
    "            \n",
    "        # first initializer\n",
    "        model_state = model_client_list[0].state_dict()\n",
    "        client_data_size = client_subset[0]['size']\n",
    "        for key in model_state:\n",
    "            model_state[key] = (client_data_size / train_len) * model_state[key]\n",
    "        \n",
    "        for c in range(1, len(model_client_list)):\n",
    "            \n",
    "            client_model_state = model_client_list[c].state_dict()\n",
    "            client_new_data_size = client_subset[c]['size']\n",
    "            \n",
    "            for key in model_state:\n",
    "                model_state[key] += (client_new_data_size / train_len) * client_model_state[key]\n",
    "                \n",
    "        averagedModel = copy.deepcopy(init_model)\n",
    "        averagedModel.load_state_dict(model_state)\n",
    "        model = copy.deepcopy(averagedModel)\n",
    "        \n",
    "        model, statistics = train_model(model, valloader, criterion, optimizer, scheduler, num_epochs=1, phase = 'val')\n",
    "        print(\"Done with validation\", statistics)\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LENET(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(LENET, self).__init__()\n",
    "        from collections import OrderedDict\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=(5, 5))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(5, 5))\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(5, 5))\n",
    "        #self.conv4 = nn.Conv2d(64, 128, kernel_size=(5, 5))\n",
    "        self.linear1 = nn.Linear(64 * 24 * 24, 120)\n",
    "        self.linear2 = nn.Linear(120, 84)\n",
    "        self.linear3 = nn.Linear(84, n_classes)                                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, 1, 28, 28): Input images.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, 10): Outputs of the network.\n",
    "        \"\"\"\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), kernel_size=2, stride=2)\n",
    "        #x = F.max_pool2d(F.relu(self.conv4(x)), kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 64 * 24 * 24)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LENET(len(class_names))\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch numero 0\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 0.6910 Acc: 0.5538\n",
      "Training complete in 0m 15s\n",
      "Done with clientelo numero x with stats:  [[0.6909572856370793], [0.5538401407074461], [], []]\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 0.6909 Acc: 0.5437\n",
      "Training complete in 0m 15s\n",
      "Done with clientelo numero x with stats:  [[0.6909368652507571], [0.5436665463731506], [], []]\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 0.6905 Acc: 0.5568\n",
      "Training complete in 0m 20s\n",
      "Done with clientelo numero x with stats:  [[0.6904630620105594], [0.5568050749711653], [], []]\n",
      "Epoch 0/0\n",
      "----------\n",
      "val Loss: 0.6904 Acc: 0.5916\n",
      "Training complete in 0m 9s\n",
      "Done with validation [[], [], [0.6904350258055187], [0.5915966386554621]]\n",
      "Running epoch numero 1\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 0.6906 Acc: 0.5996\n",
      "Training complete in 0m 21s\n",
      "Done with clientelo numero x with stats:  [[0.6905679696915197], [0.5996251441753172], [], []]\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 0.6909 Acc: 0.5896\n",
      "Training complete in 0m 15s\n",
      "Done with clientelo numero x with stats:  [[0.6909316144512341], [0.5896032831737346], [], []]\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 0.6909 Acc: 0.5830\n",
      "Training complete in 0m 16s\n",
      "Done with clientelo numero x with stats:  [[0.6909186935132267], [0.5830025261638397], [], []]\n",
      "Epoch 0/0\n",
      "----------\n",
      "val Loss: 0.6912 Acc: 0.6127\n",
      "Training complete in 0m 9s\n",
      "Done with validation [[], [], [0.6911718998636518], [0.6127450980392156]]\n",
      "Running epoch numero 2\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 0.6915 Acc: 0.5842\n",
      "Training complete in 0m 16s\n",
      "Done with clientelo numero x with stats:  [[0.6914801652135413], [0.5841983852364475], [], []]\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 0.6910 Acc: 0.6247\n",
      "Training complete in 0m 18s\n",
      "Done with clientelo numero x with stats:  [[0.6909893628066972], [0.62467464862051], [], []]\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 0.6915 Acc: 0.6086\n",
      "Training complete in 0m 18s\n",
      "Done with clientelo numero x with stats:  [[0.6914809575904644], [0.608559702950948], [], []]\n",
      "Epoch 0/0\n",
      "----------\n",
      "val Loss: 0.6919 Acc: 0.6127\n",
      "Training complete in 0m 10s\n",
      "Done with validation [[], [], [0.6919172633261907], [0.6127450980392157]]\n"
     ]
    }
   ],
   "source": [
    "model = train_fedavg_model(model, device, clients_listo, optimizer_ft, criterion, exp_lr_scheduler, C_FRACTION, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
